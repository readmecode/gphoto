# Google Drive â†’ Google Photos Auto Sync

Automated synchronization tool that transfers photos and videos from Google Drive to Google Photos using `rclone`.

Works **cloud-to-cloud**, without local downloads.

---

## ðŸš€ Features

- ðŸ”„ **Direct transfer (Drive â†’ Photos)** â€” No local downloads required
- ðŸ“… **Auto sorting by date/type** â€” Photos and videos organized into `YYYY_MM_photo` / `YYYY_MM_video` albums
- ðŸ§  **Quota-safe uploading** â€” Monitors daily Google API limits
- ðŸ—‚ **Album creation** â€” Automatically creates albums in Google Photos
- ðŸ§¾ **Detailed logs and reports** â€” Saves `.log`, `.json` and `.state` files
- âš™ï¸ **Resumable sync** â€” Automatically continues after quota/errors
- âš¡ **Optimizations**:
  - Album caching (avoids repeated existence checks)
  - Precise API request counting (accounts for different operation types)
  - Safety reserve (100 requests)
  - Proactive stop at 98% limit

## ðŸ§° Requirements

### Python Packages
```bash
pip install -r requirements.txt
```

### External Tools
```bash
brew install rclone ffmpeg jq
```

## âš™ï¸ Configuration

The project uses environment variables (via `.env` file):

| Variable | Default | Description |
|----------|---------|-------------|
| `GDRIVE_REMOTE` | `gdrive` | rclone remote name for Google Drive |
| `GPHOTOS_REMOTE` | `gphotos` | rclone remote name for Google Photos |
| `SOURCE_PATH` | `Photo` | Path in Google Drive to sync from |
| `LOG_DIR` | `~/gphoto_logs` | Directory for logs and reports |
| `MAX_PARALLEL_UPLOADS` | `2` | Maximum parallel uploads |
| `UPLOAD_TIMEOUT` | `600` | Upload timeout in seconds |
| `PHOTO_EXT` | `.jpg,.jpeg,.png,.heic,.cr2` | Photo file extensions (comma-separated) |
| `VIDEO_EXT` | `.mp4,.mov,.avi,.mkv` | Video file extensions (comma-separated) |
| `IGNORED_EXT` | `.thm,.lrv,.json` | Extensions to ignore (comma-separated) |

### Example `.env` file

```env
GDRIVE_REMOTE=gdrive
GPHOTOS_REMOTE=gphotos
SOURCE_PATH=Photo
LOG_DIR=~/gphoto_logs
MAX_PARALLEL_UPLOADS=2
UPLOAD_TIMEOUT=600
PHOTO_EXT=.jpg,.jpeg,.png,.heic,.cr2
VIDEO_EXT=.mp4,.mov,.avi,.mkv
IGNORED_EXT=.thm,.lrv,.json
```

## ðŸ”‘ How to Get Google API Keys (Client ID & Secret)

To allow `rclone` to work on your behalf, you need to create an OAuth client in Google Cloud Console.

### Step 1. Create a Project in [Google Cloud Console](https://console.cloud.google.com/projectcreate)

- Name it, for example: `rclone-photos-sync`
- Click **Create**

### Step 2. Enable APIs

Navigate to:

- **APIs & Services â†’ Library**

  Find and enable:

  - âœ… **Google Drive API**
  - âœ… **Google Photos Library API**

### Step 3. Create OAuth 2.0 Client ID

1. Go to: **APIs & Services â†’ Credentials â†’ Create Credentials â†’ OAuth client ID**
2. Application type: **Desktop App**
3. Name it, for example: `rclone-photos-sync`
4. Save â€” you'll get:
   - `Client ID`
   - `Client Secret`

**Important**: If you see "Access blocked: app not verified", add yourself as a **tester** in **OAuth consent screen â†’ Test users**.

## ðŸ”§ Configure rclone

Open terminal:

```bash
rclone config
```

This opens an interactive menu:

### 1ï¸âƒ£ Add Google Drive

```
n) New remote
name> gdrive
Storage> drive
```

- **Client ID** â†’ paste your ID
- **Client Secret** â†’ paste your Secret
- Full access (scopes): `drive`
- At the end, say `y` â€” browser will open for authorization

Verify:

```bash
rclone lsd gdrive:
```

### 2ï¸âƒ£ Add Google Photos

```
n) New remote
name> gphotos
Storage> google photos
```

- **Client ID / Secret** â†’ you can use the same ones
- When asked:

  ```
  Read only? (true/false)
  ```

  choose **false**
- During authorization, make sure redirect URL matches what rclone shows:

  ```
  http://127.0.0.1:53682/
  ```
- Confirm access in browser

Verify:

```bash
rclone lsf gphotos:
```

You should see:

```
album/
feature/
media/
shared-album/
upload/
```

## ðŸ§­ Setup

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   brew install rclone ffmpeg jq
   ```

2. **Get Google API keys** (see section above)

3. **Configure rclone** (see section above)

4. **Create `.env` file** in project root:
   ```bash
   cp .env.example .env  # if you have an example file
   # or create manually with the example from Configuration section
   ```

5. **Quick test** (see section below)

6. **Run the script:**
   ```bash
   python3 main.py
   ```

## ðŸ§ª Quick Test

Before running the full sync, verify access:

```bash
# List directories in Google Drive
rclone lsd gdrive:

# List directories in Google Photos
rclone lsd gphotos:

# Test copy (dry-run)
rclone copy gdrive:Photo/test.jpg gphotos:upload --dry-run --progress
```

If all commands work without errors, you're ready to proceed.

## ðŸ”„ How It Works

1. **File Discovery**: Lists all files from Google Drive using `rclone lsjson`
2. **Filtering**: Filters files by extension (photos/videos only)
3. **Album Detection**: Extracts date from filename using regex pattern `(19|20\d{2})[-_.]?(0[1-9]|1[0-2])`
4. **Upload**: Transfers files via `rclone copy` with quota checks
5. **State Management**: Saves progress to `state.json` for idempotency
6. **Reporting**: Logs progress every 100 files with ETA and quota usage

## ðŸ•’ Quota Management

The script implements sophisticated quota management to prevent exceeding Google Photos API limits:

- **API Quota**: 10,000 requests per day
- **Upload Quota**: 50 GB per day
- **Warning Threshold**: 80% usage
- **Critical Threshold**: 90% usage
- **Stop Threshold**: 98% usage (with 100 request safety reserve)

The script tracks:
- API requests (estimated per operation: 2 for upload, 3 for first file in album)
- Uploaded bytes
- Automatic quota reset detection (midnight PST)

When quota is exceeded, the script:
- Saves current state
- Logs reset time
- Provides instructions for resuming after quota reset

## ðŸ“ Output Files

All output files are saved to `LOG_DIR` (default: `~/gphoto_logs/`):

- `sync_YYYYMMDD_HHMMSS.log` - Detailed operation log
- `summary_YYYYMMDD_HHMMSS.json` - Final report with metrics:
  - Processed and uploaded file counts
  - Created albums list
  - Quota usage (API requests and bytes)
  - Execution duration
  - Quota limit information (if reached)
- `state.json` - Sync state (processed files)
- `daily_quota.json` - Daily quota tracking

## ðŸ›¡ï¸ Error Handling

The script automatically handles:
- **Rate limiting (429 errors)**: Exponential backoff with retries
- **Daily quota exceeded**: Graceful stop with state preservation
- **Temporary errors**: Automatic retries with increasing delays
- **Network issues**: Configurable retries and timeouts

## ðŸ“Š Example Output

```
[10:30:15] ðŸ“„ Ð›Ð¾Ð³: ~/gphoto_logs/sync_20240115_103015.log
[10:30:15] ðŸ“Š Ð¢ÐµÐºÑƒÑ‰Ð¸Ðµ ÐºÐ²Ð¾Ñ‚Ñ‹: 0/10000 Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð², 0.0 ÐœÐ‘ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾
[10:30:16] ðŸ“‚ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ„Ð°Ð¹Ð»Ð¾Ð²: 1250
[10:30:20] âœ… IMG_2023_05_15.jpg â†’ 2023_05_photo
[10:30:25] âœ… VID_2023_06_20.mp4 â†’ 2023_06_video
...
[10:45:30] ðŸ“ˆ ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ: 100/1250 (8.0%) ETA â‰ˆ 125.5 Ð¼Ð¸Ð½ | ÐšÐ²Ð¾Ñ‚Ñ‹: 200/10000 Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð², 1250.5 ÐœÐ‘
```

## ðŸ”„ Resuming After Quota Exceeded

If the script stops due to quota limits:

1. Wait until the reset time (shown in logs, typically midnight PST / 10:00 Turkey time)
2. Run the script again:
   ```bash
   python3 main.py
   ```
3. The script will automatically resume from where it stopped using `state.json`

ðŸ“… Quota resets at **00:00 PST (10:00 Turkey time)**

## License

This project is provided as-is for personal use.

## ðŸ©º Troubleshooting

| Problem | Solution |
|---------|----------|
| `Quota exceeded for quota metric 'all requests per day'` | Wait until 10:00 Turkey time (00:00 PST) â€” quota will reset |
| `rclone: can't upload files here` | Use `gphotos:upload` or `gphotos:album/...`, not just `gphotos:` |
| `directory not found` | Verify path exists: `rclone lsf gdrive:Photo` |
| `Access blocked: app not verified` | Add yourself as **tester** in Google Cloud â†’ OAuth consent screen â†’ Test users |
| `401: Invalid Credentials` | Re-authenticate: `rclone config reconnect gdrive:` and `rclone config reconnect gphotos:` |
| `403: Forbidden` | Check that APIs are enabled in Google Cloud Console |

## â˜ï¸ Multi-Project Hack (Bypass Daily Limits)

Want to upload more than 10,000 files per day?

Create multiple projects:

```
rclone-photos-sync-1
rclone-photos-sync-2
rclone-photos-sync-3
```

In each, enable Photos API and configure separate `client_id`.

In `.rclone.conf` you can quickly switch:

```ini
[gphotos1]
type = google photos
client_id = ...
client_secret = ...
token = {...}

[gphotos2]
type = google photos
client_id = ...
client_secret = ...
token = {...}
```

Then upload by years:

```bash
rclone copy gdrive:Photo/2010 gphotos1:upload
rclone copy gdrive:Photo/2011 gphotos2:upload
```

## ðŸ“… Automation (cron)

For automatic daily runs:

```bash
crontab -e
```

Add:

```bash
# Run at 10:05 AM daily (after quota reset at 10:00)
5 10 * * * /usr/local/bin/python3 /path/to/gphoto/main.py >> ~/gphoto_cron.log 2>&1
```

Adjust the path to match your installation.

## âœ… Final Check

After running the sync, verify results:

```bash
# List created albums
rclone lsf gphotos:album/ --dirs-only | head

# Check latest log
tail -n 20 ~/gphoto_logs/sync_*.log
```

If you see lines like:

```
âœ… IMG_2020_07_01.jpg â†’ 2020_07_photo
ðŸŽ‰ Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾: 1250 Ñ„Ð°Ð¹Ð»Ð¾Ð², Ð¾ÑˆÐ¸Ð±Ð¾Ðº 0.
```

â€” everything is working perfectly ðŸ’ª

## Notes

- The script uses PST timezone for quota reset calculations
- Album names are derived from file names using regex pattern matching
- Files already processed are skipped (tracked in `state.json`)
- The script is designed for large collections and can handle interruptions gracefully
- The script automatically creates albums in format `YYYY_MM_photo` / `YYYY_MM_video` based on file dates

